\documentclass[a4paper, 11pt]{article}

\setlength{\oddsidemargin}{0in}
\setlength{\textwidth}{6.5in}
\setlength{\topmargin}{-0.5in}
\setlength{\textheight}{9in}

\usepackage[T1]{fontenc}
\usepackage[cp1251]{inputenc}
\usepackage[english, russian]{babel}
\usepackage[normalem]{ulem}  % для зачекивания текста
\usepackage{amsmath,amssymb}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{color}
\usepackage{dcolumn}
\usepackage{bm}
\usepackage{float}


\newcommand{\expnumber}[2]{{#1}\mathrm{e}{#2}}
\newcommand{\doublenorm}[1]{\left\lVert #1 \right\rVert}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{pdfpages}

\usepackage[colorlinks=true, linkcolor=black, citecolor=black, urlcolor=black]{hyperref}

\setcounter{tocdepth}{2}

\newcommand{\SSA}{\textbf{SSA}}
\newcommand{\GSSA}{\textbf{GSSA}}
\newcommand{\CISSA}{\textbf{CiSSA}}
\newcommand{\TS}{\mathsf{X}}

\newtheorem{definition}{Определение} % задаём выводимое слово (для определений)
\newtheorem{theorem}{Теорема} % задаём выводимое слово (для определений)
\newtheorem{comment}{Замечание} % задаём выводимое слово (для определений)

%\usepackage{fleqn}


%##################


\date{}
\begin{document}

%\input{../Титульник/report.tex}
\includepdf[pages=-]{../Title/report.pdf}

\tableofcontents
\noindent
\textbf{7\textit{} \space Список литературы}

\newpage

\section{Введение}


Временные ряды представляют собой упорядоченную последовательность данных, собранных или измеренных в хронологическом порядке. Они играют ключевую роль в анализе и прогнозировании различных явлений в таких областях, как экономика, финансы, климатология и медицина. Понимание эволюции этих явлений во времени критично для выявления тенденций, циклов и аномалий.

Для уточнения терминологии, следует отметить, что \textbf{временной ряд длины \( N \)} представляет собой упорядоченную конечную последовательность значений, которая записывается как \( \TS = (x_1, \dots, x_{N}) \), где \( N > 2 \). Одним из основных аспектов анализа временных рядов является разделение их на составляющие компоненты. Среди таких компонентов важными являются \textbf{тренд}, который отражает медленно изменяющуюся долгосрочную динамику ряда, и \textbf{сезонность}, представляющая собой периодические колебания, вызванные повторяющимися факторами, такими как климатические или экономические циклы.

Для эффективного анализа и понимания структуры временных рядов разработаны различные методы, позволяющие разделить ряд на его компоненты. Существует два вида разделимости: \textbf{точная разделимость}, которая характеризует способность метода точно выделять отдельные компоненты ряда, и \textbf{ассимптотическая разделимость}, которая описывается следующим образом:

\begin{definition}
	\label{def:asymp}
	Есть метод разделения ряда на компоненты с параметрами \( \Theta \), ряд \( \TS = \TS^{(1)} + \TS^{(2)} \). Существуют такой фиксированный набор параметров \( \hat{\Theta} \) и последовательность \( L = L(N) \), \( N \rightarrow \infty \), что при разделении ряда на компоненты этим методом, \( \hat{\TS}^{(1)} \) --- отвечает за \( \TS^{(1)} \), при этом, \( \mathrm{MSE}\left(\TS^{(1)}, \hat{\TS}^{(1)}\right) \rightarrow 0 \), где \( \mathrm{MSE} \) --- среднеквадратическая ошибка. Тогда ряды \( \TS^{(1)} \) и \( \TS^{(2)} \) называются асимптотически \( L(N) \)-разделимыми данным методом.
\end{definition}

\begin{comment}
	Для $\TS^{(2)}$ своя компонента ряда $\hat \TS^{(2)}$, для которой также будет выполнено $\mathrm{MSE}\left(\TS^{(2)}, \hat \TS^{(2)}\right) \rightarrow 0$.
\end{comment}

Методы разделения временных рядов играют ключевую роль в выделении тренда, сезонности и других структурных компонентов, что позволяет глубже понять и моделировать временные зависимости.



Сингулярный спектральный анализ ($\SSA$ \cite{golyandina2001analysis}) --- метод, целью которого является разложение оригинального ряда на сумму небольшого числа интерпретируемых компонентов, таких как медленно изменяющаяся тенденция (тренд), колебательные компоненты (сезонность) и ``структурный'' шум. Основной концепцией при изучении свойств методов $\SSA$ является ``разделимость'', которая характеризует, насколько хорошо разные компоненты могут быть отделены друг от друга.

В данном исследовании рассматривается математическая составляющая вариации алгоритма $\SSA$ --- circulant singular spectrum analysis ($\CISSA$), предложенная в статье \cite{bogalo2020}, а также сравнение базового метода и циркулярного, применение их на языке R.

\begin{enumerate}
	\item Ознакомиться с алгоритмом $\CISSA$;
	\item Реализовать алгоритм $\CISSA$ на языке R; 
	\item Сравнить алгоритмы $\SSA$, разложение Фурье и $\CISSA$.
\end{enumerate}

Кроме того, рассмотрен алгоритм $\GSSA$, предложенный в статье \cite{gu2024generalized}, показана его смысловая ценность с точки зрения линейных фильтров и указаны случаи, когда такой алгоритм будет предпочтительнее обычного $\SSA$.

Далее кратко опишем структуру работы. В разделе \ref{sec:ssa} рассматривается базовый метод $\SSA$ и его ключевые свойства. В следующем разделе \ref{sec:cissa} представлен метод $\CISSA$, также с описанием его основных характеристик. В секции \ref{sec:gssa} показан $\GSSA$. Раздел \ref{sec:comparison} посвящён сравнению методов $\SSA$, разложения Фурье и $\CISSA$ на модельных и реальных примерах. В заключительной секции \ref{sec:concl} подведены основные итоги исследования.





\newpage

\section{Базовый метод SSA}
\label{sec:ssa}


Рассмотрим базовый метод сингулярного спектрального анализа \cite{golyandina2001analysis}.

\subsection{Алгоритм метода SSA}

Пусть $N > 2$, вещественнозначный временной ряд
$\TS = (x_1, \dots, x_{N})$ длины $N$.
Базовый алгоритм состоит $\SSA$ из четырех шагов.

\subsubsection{Вложение}
$L$ --- некоторое целое число (длина окна), $1 < L < N$. Строится $L$-траекторная матрица $\mathbf{X}$, состоящая из $K = N - L + 1$ векторов вложения:
\begin{equation}
	\label{eq:X}
	\mathbf{X} = 
	\begin{pmatrix}
		x_1 & x_2 & x_3 & \dots & x_{K} \\
		x_2 & x_3 & x_4 & \dots & x_{K+1} \\
		x_3 & x_4 & x_5 & \dots & x_{K+2} \\
		\vdots & \vdots & \vdots & \ddots & \vdots \\
		x_{L} & x_{L+1} & x_{L+2} & \dots & x_{N}
	\end{pmatrix}.
\end{equation}
Полезным свойством является то, что матрица $\mathbf{X}$ имеет одинаковые элементы на антидиагоналях. Таким образом, $L$-траекторная матрица является ганкелевой.

\subsubsection{Сингулярное разложение (SVD)}
Результатом этого шага является сингулярное разложение (Singular Value Decomposition, $\mathbf{SVD}$)  траекторной матрицы ряда.

Пусть $\mathbf{S} = \mathbf{X}\mathbf{X}^{\mathrm{T}}$, 
$\lambda_1, \dots, \lambda_L$ --- собственные числа матрицы $\mathbf{S}$, взятые в неубывающем порядке и
$U_1, \dots, U_L$ --- ортонормированная система собственных векторов соответствующих собственным числам матрицы $\mathbf S$. 

Определим $d = \max{ \{i: \lambda_i > 0 \}}$ и 
$V_i = \mathbf{X}^{\mathrm{T}} U_i / \sqrt{\lambda_i}$.
Тогда сингулярным разложением называется представление матрицы в виде:
\begin{equation}
	\mathbf{X} = \mathbf{X}_1 + \dots + \mathbf{X}_d =
	\sum_{i = 1}^{d} \sqrt{\lambda_i} U_i V_{i}^{\mathrm{T}}\label{eq:1}.
\end{equation}

Набор $( \sqrt{\lambda_i}, U_i, V_{i}^{\mathrm{T}})$ называется $i$-й собственной тройкой разложения \eqref{eq:1}.

\subsubsection{Группировка}
На основе разложения \eqref{eq:1} производится процедура группировки, которая делит все множество индексов $\{1, \dots, d\}$ на $m$ непересекающихся подмножеств $I_1, \dots, I_d$. 

Пусть $I = \{i_1, \dots, i_p\}$, тогда $\mathbf{X}_I =
\mathbf{X_{i_1}} + \dots + \mathbf{X_{i_p}}$. Такие матрицы вычисляются для каждого $I = I_1, \dots, I_m$. 
В результате получаются матрицы $\mathbf{X_{I_1}}, \dots, \mathbf{X_{I_m}}$. Тем самым разложение \eqref{eq:1} может быть записано в сгруппированном виде:
\begin{equation*}
	\mathbf{X} = \mathbf{X}_{I_1} + \dots + \mathbf{X}_{I_m}.
\end{equation*}

\subsubsection{Диагональное усреднение}
Пусть $\mathbf{Y}$ --- матрица размерности $L \times K$. $L^* = \min(L, K), \, K^* = \max(L,K)$ Диагональное усреднение переводит матрицу $\mathbf{Y}$ в временной ряд $g_0, \dots, g_{N-1} $:

\begin{equation*}
	g_{k}=
	\begin{cases}
		\frac{1}{k+1} \sum\limits_{m=1}^{k+1} y_{m,k-m+2}^{*} &
		 \text{для } 0 \leq k < L^* - 1, \\
		
		\frac{1}{L^{*}} \sum\limits_{m=1}^{L^*} y_{m,k-m+2}^{*} &
		 \text{для } L^*-1 \leq k < K^* , \\
		
		\frac{1}{N-k} \sum\limits_{m=k-K^*+2}^{N-K^*+1} y_{m,k-m+2}^{*} &
		\text{для } K^* \leq k < N .\\
	\end{cases}
\end{equation*}
Применяя данную операцию к матрицам $\mathbf{X_{I_1}}, \dots, \mathbf{X_{I_m}}$, получаются $m$ новых рядов: $\TS_1, \dots, \TS_m$. При этом, $\TS_1 + \dots + \TS_m = \TS$.

\subsection{Свойства SSA}

\subsubsection{Точная разделимость}


Пусть временной ряд  $\TS = \TS^{(1)} + \TS^{(2)}$ и задачей является нахождение этих слагаемых. В результате базового алгоритма $\SSA$ также получаем $2$ ряда. Возникает вопрос: в каких случаях мы можем так выбрать параметр алгоритма $L$ при $m = 2$ и так сгруппировать собственные тройки, чтобы получить исходные 2 ряда без смешиваний?
При выборе длины окна L, каждый из рядов $\TS^{(1)}$, $\TS^{(2)}$, $\TS$ порождает траекторную матрицу $\mathbf{X}^{(1)}, \mathbf{X}^{(2)}, \mathbf{X}$.

\begin{definition}
	Будем говорить, что ряды $\TS^{(1)}$ и $\TS^{(2)}$ слабо L-разделимы, если пространства, порождаемые строками $\mathbf{X}^{(1)}$ и $\mathbf{X}^{(2)}$ соответственно, ортогональны. То же самое должно выполняться для столбцов \cite{golyandina2001analysis}.
\end{definition}

Если выполняется условие слабой L-разделимости, тогда существует такое сингулярное разложение траекторной матрицы $\mathbf X$ ряда $\TS$, что его можно разбить на две части, являющиеся сингулярными разложениями траекторных матриц рядов $\TS^{(1)}, \TS^{(2)}$ \cite{golyandina2001analysis}.

\begin{definition}
	Будем говорить, что ряды $\TS^{(1)}, \TS^{(2)}$ сильно L-разделимы, если они слабо L-разделимы и после процедуры $\mathbf{SVD}$ собственные числа рядов различны \cite{golyandina2001analysis}.
\end{definition}

Если выполняется условие сильной L-разделимости, тогда любое сингулярное разложение траекторной матрицы $\mathbf X$ ряда $\TS$ можно разбить на две части, являющиеся сингулярными разложениями траекторных матриц рядов $\TS^{(1)}, \TS^{(2)}$ \cite{golyandina2001analysis}.


Рассмотрим таблицу, в которой знаком + отмечены пары рядов, для которых существуют параметры функций и параметры метода $L$ и $ K = N - L +1$, при которых они разделимы (точно разделимы). Данная таблица \ref{tab:1} и условия разделимости с доказательствами взяты из книги \cite{golyandina2001analysis}.

\begin{table}[H]
	\begin{center}
		\caption{Точная разделимость}
		\label{tab:1}
		\scalebox{1}{
			\begin{tabular}{cccccc}
				\hline
				& const & cos & exp & exp cos & ak+b \\ \hline
				const   & -     & +   & -   & -       & -    \\
				cos     & +     & +   & -   & -       & -    \\
				exp     & -     & -   & -   & +       & -    \\
				exp cos & -     & -   & +   & +       & -    \\
				ak+b    & -     & -   & -   & -       & -    \\ \hline
		\end{tabular}}
	\end{center}
\end{table}

Стоит отметить, что точная разделимость для $\cos$ достигается, если $Lw \in \mathbb{N}, \, Kw \in \mathbb{N}$, где $w$ --- частота \cite{golyandina2001analysis}.

Однако, по таблице \ref{tab:1} видно, что условия точной разделимости достаточно жесткие и вряд ли выполнимы в реальных задачах. Тогда появляется такое понятие, как асимптотическая разделимость.

\subsubsection{Асимптотическая разделимость}

Для любого ряда $\TS$ длины $N$ определим
$\TS_{i,j}\,=\,(x_{i-1},\cdot\cdot\cdot,x_{j-1}),\;\;1\,\leq\,i\,\leq\,j\,<\,N.$
Пусть $\TS^{(1)}=(x_{0}^{(1)},\ldots,x_{N-1}^{(1)}),\TS^{(2)}=(x_{0}^{(2)},\ldots,x_{N-1}^{(2)}).$ Тогда определим коэффициент корреляции следующим образом:


\begin{equation*}
	\rho_{i,j}^{(M)}=\frac{\left(\TS_{i,i+M-1}^{(1)},\TS_{j,j+M-1}^{(2)}\right)}{\left|\left|\TS_{i,i+M-1}^{(1)}\right|\right|\left|\left|\TS_{j,j+M-1}^{(2)}\right|\right|}.
\end{equation*}

\begin{definition}
	Ряды $\TS^{(1)}, \TS^{(2)}$ называются $\varepsilon$-разделимыми при длине окна $L$, если
	\begin{equation*}
		\rho^{(L,K)}\ {\stackrel{\mathrm{def}}{=}}\ \mathrm{max}\left(\operatorname*{max}_{1\leq i,j\leq K}|\rho_{i,j}^{(L)}|,\operatorname*{max}_{1\leq i,j\leq L}|\rho_{i,j}^{(K)}|\right)<\varepsilon
		\text{  \cite{golyandina2001analysis}.}
	\end{equation*}
	
\end{definition}

\begin{definition}
	Если $\rho^{(L(N),K(N))} \rightarrow 0$ при некоторой последовательности $L = L(N) $, $N \rightarrow \infty$, то ряды $\TS^{(1)}, \TS^{(2)}$ называются асимтпотически $L(N)$-разделимыми \cite{golyandina2001analysis}.
\end{definition}

Как можно заметить по таблице \ref{tab:2}, для гораздо большего класса функций асимптотическая разделимость имеет место \cite{golyandina2001analysis}.
\begin{table}[H]
	\begin{center}
		\caption{Асимптотическая разделимость}
		\label{tab:2}
		\scalebox{1}{
			\begin{tabular}{cccccc}
				\hline
				& const & cos & exp & exp cos & ak+b \\ \hline
				const   & -     & +   & +   & +       & -    \\
				cos     & +     & +   & +   & +       & +    \\
				exp     & +     & +   & +   & +       & +    \\
				exp cos & +     & +   & +   & +       & +    \\
				ak+b    & +     & +   & +   & +       & -    \\ \hline
		\end{tabular}}
	\end{center}
\end{table}

\subsubsection{Алгоритмы улучшения разделимости}
Для $\SSA$ существуют алгоритмы улучшения разделимости. Они позволяют более точно отделять временные подряды друг от друга. В данной работе будут использоваться методы EOSSA и FOSSA. Подробнее про них можно почитать в \cite{golyandina2023intelligent}. Для нас важно, что благодаря применению улучшения разделимости мы можем делать автоматическую группировку по заданным частотам в базовом алгоритме $\SSA$.

\subsubsection{SSA как линейный фильтр}
Разложение временного ряда методом $\SSA$ можно интерпретировать как применение линейных фильтров. Для дальнейшего исследования введем следующие определения.

\begin{definition}
	Рассмотрим бесконеченый временной ряд $\TS = (\dots, x_{-1}, x_0, x_1, \dots)$. Линейный конечный фильтр --- это оператор $\Phi$, который преобразует временной ряд $\TS$ в новый по следующему правилу:
	\begin{equation*}
		y_j = \sum \limits_{i = -r_1}^{r_2} h_i x_{j-i}; \quad r_1, r_2 < \infty.
	\end{equation*}
	Набор коэффициентов ${h_i}$ --- импульсная характеристика фильтра.
\end{definition} 

Там, где не оговорено обратного, будем называть линейный конечный фильтр просто линейным фильтром. 

\begin{definition}
	Передаточная функция линейного фильтра $\Phi$:
	\begin{equation*}
		H_{\Phi}(z) = \sum \limits_{i = -r_1}^{r_2} h_i z^{-i}.
	\end{equation*}
\end{definition}

\begin{definition}
	Амплитудно-частотная характеристика (АЧХ) линейного фильтра $\Phi$:
	\begin{equation*}
		A_{\Phi}(\omega) = \left| H_{\Phi}\left(e^{i2\pi\omega}\right) \right|.
	\end{equation*}
\end{definition}

АЧХ фильтра  — это график или функция, которая показывает, как фильтр изменяет амплитуды (силу) разных частот входного сигнала.

Теперь рассмотрим алгоритм $\SSA$ с точки зрения линейных фильтров \cite{golyandina2020singular}.
Пусть $\TS = (x_1, \dots, x_{N})$ --- временной ряд длины $N$, $K = N - L + 1, \quad L^{*} = \min(L, K)$. Пусть $L$ будет длиной окна, а $(\sqrt{\lambda},\,U,\,V)$ — одной из собственных троек. Определим диагональную матрицу $N \times N$:
$$
\mathbf{D} = \text{diag}(1, 2, 3, \ldots, L^{*}-1, L^{*}, L^{*}, \ldots, L^{*}, L^{*}-1, \ldots, 2, 1)
$$
и матрицу  $K \times N$
\[
\mathbf{W} = \begin{pmatrix}
	u_{1} & u_{2} & u_{3} & \cdots & u_{L} & 0 & \cdots & 0 & 0 & 0 \\
	0 & u_{1} & u_{2} & u_{3} & \cdots & u_{L} & 0 & \cdots & 0 & 0 \\
	\vdots & 0 & \ddots & \ddots & \ddots & \cdots & \ddots & 0 & \cdots & 0 \\
	0 & \cdots & 0 & u_{1} & u_{2} & u_{3} & \cdots & u_{L} & 0 & \vdots \\
	0 & 0 & \cdots & 0 & u_{1} & u_{2} & u_{3} & \cdots & u_{L} & 0 \\
	0 & 0 & 0 & \cdots & 0 & u_{1} & u_{2} & u_{3} & \cdots & u_{L}
\end{pmatrix}.
\]
Здесь $U = (u_1, \dots, u_L)$ --- собственный вектор матрицы $\mathbf{S}$.
\begin{theorem}
	\label{th:filter_SSA}
	Компонента временного ряда $\widetilde \TS$, восстановленная с использованием собственной тройки $(\sqrt{\lambda},\,U,\,V)$, имеет вид:
	\[
	\widetilde{\TS}^{{\rm T}} = \mathbf{D}^{-1}\mathbf{W}^{{\rm T}}\mathbf{W}\TS^{\rm T}.
	\]
\end{theorem}
\begin{proof}
	Доказательство можно найти в \cite{golyandina2020singular} (неплохо бы расписать).
\end{proof}

Таким образом, для восстановления методом $\SSA$ средних точек (индексы от $L$ до $K$) имеем следующий фильтр:
\begin{equation}
	{\widetilde{x}}_{s} = \sum_{j=-(L-1)}^{L-1} \left( \sum_{k=1}^{L-|j|} u_{k} u_{k+|j|} / L \right) x_{s-j}, \quad L \leq s \leq K.
\end{equation}
Похожим образом можно переписать $\SSA$ через линейные фильтры для точек в начале и конце.


%Поскольку при применении улучшения разделимости можно сделать автоматическую группировку по заданным частотам, то введено в рассмотрение следующее определение:

%\begin{definition}
%	\label{def:asymp}
%	Есть метод разделения ряда на компоненты с параметрами $\Theta$, ряд $\TS = \TS^{(1)} + \TS^{(2)}$. Существуют такой фиксированный набор параметров $\hat \Theta$ и последовательность $L = L(N) $, $N \rightarrow \infty$, что при разделении ряда на компоненты этим методом, $\hat \TS^{(1)}$ --- отвечает за $\TS^{(1)}$, при этом, $\mathrm{MSE}\left(\TS^{(1)}, \hat \TS^{(1)}\right) \rightarrow 0$, где $MSE$ --- среднеквадратическая ошибка. Тогда ряды $\TS^{(1)}$ и $ \TS^{(2)}$ называются асимтпотически $L(N)$-разделимыми данным методом.
%\end{definition}
%
%\begin{comment}
%	Для $\TS^{(2)}$ также будет своя компонента ряда $\hat \TS^{(2)}$, для которой будет выполнено $\mathrm{MSE}\left(\TS^{(2)}, \hat \TS^{(2)}\right) \rightarrow 0$.
%\end{comment}

%\begin{comment}
%	Для $\SSA$ параметры $\Theta$ --- это набор компонент. Для $\CISSA$ --- набор частот.
%\end{comment}



\newpage

\section{Метод Circulant singular spectrum analysis (CiSSA)}
\label{sec:cissa}

%* Переформулировка начала секции 3 из статьи про $\CISSA$ от третьего лица (сказать зачем он создавался, т.е. для автоматического выделения частот)

В этом разделе описана модификация $\SSA$ на основе циркулярной матрицы \cite{bogalo2020}. Авторы метода называют её автоматизированной. Причем автоматизированная в том смысле, что компоненты ряда группируются по частотам самим алгоритмом. Сначала будет рассмотрен метод только для стационарного случая, затем показана его применимость при использовании нестационарного ряда.

Стационарность подразумевает неизменность статистических свойств ряда во времени. Определим это понятие формально \cite{golyandina2001analysis}.  
\begin{definition}
		 Пусть $\TS = (x_1, \dots, x_n, \dots)$ — временной ряд. Ряд $\TS$ называется стационарным, если существует функция $R_{\TS}(k)$ ($-\infty < k < +\infty$) такая, что для любых $k, l \geq 1$  
		 \begin{equation}
			R_{\TS}^{(N)}(k, l) \overset{\mathrm{def}}{=} \frac{1}{N} \sum_{m=1}^{N} x_{k+m} x_{l+m} \xrightarrow{N \to \infty} R_{\TS}(k - l). \label{eq:R}	 	
		 \end{equation}
	
	Если \eqref{eq:R} выполняется, тогда $R_{\TS}$ называется ковариационной функцией стационарного ряда $\TS$.
\end{definition}

\begin{theorem}
	Пусть $R_{\TS}$ — ковариационная функция стационарного ряда $\TS$. Тогда существует конечная мера $m_{\TS}$, определенная на борелевских подмножествах $(-1/2, 1/2]$, такая, что  
	\[
	R_{\TS}(k) = \int_{(-\frac{1}{2}, \frac{1}{2}]} e^{i 2 \pi k \omega} m_{\TS}(d\omega).
	\]
	
	
	Мера $m_{\TS}$ называется спектральной мерой ряда $\TS$.
\end{theorem}
\begin{proof}
	Доказательство в \cite{golyandina2001analysis}.
\end{proof}

%\begin{definition}
%	 ряд $\TS = (x_1, x_2, x_3, \dots)$ называется стационарным, если:
%	\begin{enumerate}
%		\item $\mathrm E (x_t) \equiv \mathrm{const}, \, \forall t \in 1:N$;
%		\item $\mathrm{Cov}(x_t, x_{t+h}) \equiv \mathrm{const}$ при фиксированном h.
%	\end{enumerate}
%\end{definition}

\subsection{Алгоритм метода CiSSA}
%План:
%\begin{enumerate}
%	\item Алгоритм в текстовом виде.
%	\item ??? Алгоритм на псевдокоде.
%	\item Кратко указать, почему он работает (сослаться на доказательства в статье)
%	\item Пояснить, что делать с нестационарными рядами, показать расширение ряда
%	\item Кратко упомянуть, что алгоритм был реализован на языке R, сослаться на код в GitHub.
%\end{enumerate}

Данный алгоритм состоит также из четырех основных шагов.

Зафиксируем стационарный временной ряд $\TS$ состоящий из $N$ элементов и выберем длину окна $L$.
\subsubsection{Вложение}
Такой же, как и в $\SSA$. Считаем матрицу $\mathbf{X}$, заданную в \eqref{eq:X}.

\subsubsection{Разложение}
Будем рассматривать временной ряд как выборку после эксперимента, а не как случайную величину. Соответственно, все формулы будут выборочными.

Определим автоковарицации: 
\begin{equation*}
	\hat{\gamma}_m = \frac{1}{N-m} \sum \limits_{t = 1}^{N-m}x_t x_{t+m}, \, m = 0:L-1.
\end{equation*}
На основе $\hat{\gamma}_m$ определим матрицу:
\begin{equation}
	\label{eq:tepl_mat}
	\hat{\gamma}_{L}=\left(\begin{array}{cccc}
		\hat{\gamma}_{1} & \hat{\gamma}_{2} & \ldots & \hat{\gamma}_{L} \\
		\hat{\gamma}_{2} & \hat{\gamma}_{1} & \ldots & \hat{\gamma}_{L-1} \\
		\vdots & \vdots & \vdots & \vdots \\
		\hat{\gamma}_{L} & \hat{\gamma}_{L-1} & \hdots & \hat{\gamma}_{1}
	\end{array}\right).
\end{equation}
Данная матрица $L \times L$ называется Теплицевой и используется в методе Toeplitz SSA (подробнее про данный метод можно прочитать в книге \cite{golyandina2001analysis}). На ее основе составим циркулярную матрицу для алгоритма Circulant SSA \cite{bogalo2020}:


\begin{equation}
	\label{eq:circ_mat}
	\hat{\mathrm{C}}_{L}=\left(\begin{array}{cccc}
		\hat c_{1} & \hat c_{2} & \ldots & \hat c_{L} \\
		\hat c_{2} & \hat c_{1} & \ldots & \hat c_{L-1} \\
		\vdots & \vdots & \vdots & \vdots \\
		\hat c_{L} & \hat c_{L-1} & \hdots & \hat c_{1}
	\end{array}\right),
\end{equation}
где $\hat c_m = \frac{L-m}{L}\hat{\gamma}_m + \frac{m}{L}\hat{\gamma}_{L-m}, \, m = 0:L-1$.
Собственные числа матрицы $\hat{\mathrm{C}}_{L}$, определенной в \eqref{eq:circ_mat} задаются по формуле:
\begin{equation*}
	\lambda_{L,k}=\sum_{m=0}^{L-1}\hat c_{m}\exp\left(i 2\pi m\frac{k-1}{L}\right), \, k = 1:L, \, \text{причем} \, \lambda_{L,k} = \lambda_{L,L+2-k},
\end{equation*}
а собственные вектора, связанные с $\lambda_{L, k}$ вычисляются следующим образом:
\begin{equation*}
	{U}_{k}=L^{-1/2}(u_{k,1\cdot}\cdot\cdot\cdot,u_{k,L}), \, \text{где} \, 
	u_{k,j}=\exp\left(-\mathrm{i}2\pi\d(j-1)\frac{k-1}{L}\right), \,
	\text{причем} \, U_{k} = U_{L+2-k}^*,
\end{equation*}
где $U^*$ --- комплексное сопряжение вектора $U$.


\paragraph{Элементарное разложение \newline}

Для каждой частоты $w_k = \frac{k-1}{L}$, $k = 2:\lfloor \frac{L+1}{2} \rfloor$, есть два собственных вектора: $U_k$ и $U_{L+2-k}$. За частоту $w_0$ отвечает один собственный вектор --- $U_0$. Если же $L$ --- четное, то частоте $w_{\frac{L}{2} + 1}$ будет соответствовать один вектор $U_{\frac{L}{2}+1}$.

Следовательно, индексы группируются следующим образом:
\begin{equation*}
	B_1 = \{1\}; \, B_k = \{k, L+2-k\}, \,  \text{для } k = 2:\lfloor \frac{L+1}{2}\rfloor; \, 
	B_{\frac{L}{2} + 1} = \left\{ \frac{L}{2} + 1 \right\}, \, \text{если} \, L\mod 2 = 0.
\end{equation*}
%Разложение $\mathbf X_{B_k} = \mathbf X_k + \mathbf X_{L+2-k} = U_k U_k^H \mathbf X + U_{L+2-k} U_{L+2-k}^H \mathbf X$, где $U^H$ --- это комплексное сопряжение и транспонирование вектора $U$.


\subsubsection{Группировка} 
Такой же шаг, как и в базовом $\SSA$. Однако группировка будет производиться на непересекающиеся подгруппы по частотам от $0$ до $0.5$, поскольку частоты выше 0.5 представляют собой зеркальное отражение частот ниже 0.5. Именно поэтому объединяются матрицы $\mathbf X_{B_k} = \mathbf X_k + \mathbf X_{L+2-k}$. Разложение $\mathbf X_{B_k} = \mathbf X_k + \mathbf X_{L+2-k} = U_k U_k^H \mathbf X + U_{L+2-k} U_{L+2-k}^H \mathbf X$, где $U^H$ --- это комплексное сопряжение и транспонирование вектора $U$.

\subsubsection{Диагональное усреднение}
Такой же шаг, как и в базовом $\SSA$.

\begin{comment}
	\label{comm:proector}
	 $U_k U_k^H + U_{L+2-k} U_{L+2-k}^H$ является оператором проектирования на подпространство, которое порождено синусами и косинусами с частотой $w_k = \frac{k-1}{L}$. Это пространство соответствует компонентам синусоидальной структуры временного ряда, связанных с конкретной частотой, выделяемой методом. 
\end{comment}
\begin{proof}
	Рассмотрим на примере одного вектора-столбца $X_i = \left(x_i, \dots, x_{i+L}\right)^{\mathrm T}$, где $i = 1, \dots, K$. Возьмем для наглядности $i = 1$.
	$$
	U_k = L^{-\frac{1}{2}}\left(1, e^{-i2\pi \frac{k-1}{L}}, e^{-i2\pi 2\frac{k-1}{L}}, \dots, e^{-i2\pi (L-1)\frac{k-1}{L}}\right)^{\mathrm T},
	$$
	$$
	U_k^H = L^{\frac{1}{2}}\left(1, e^{i2\pi \frac{k-1}{L}}, e^{i2\pi 2\frac{k-1}{L}}, \dots, e^{i2\pi (L-1)\frac{k-1}{L}}\right).
	$$
	$$
	L^{-\frac{1}{2}}c_k = U_k^H X_1 = x_1 + e^{i2\pi \frac{k-1}{L}} x_2 + e^{i2\pi 2\frac{k-1}{L}} x_3 + \dots + e^{i2\pi (L-1)\frac{k-1}{L}} x_L.
	$$
	$$
	X_1^k = c_k U_k = \left(c_k, c_k e^{-i2\pi \frac{k-1}{L}}, c_k e^{-i2\pi 2\frac{k-1}{L}}, \dots, c_k e^{-i2\pi (L-1)\frac{k-1}{L}}\right)^{\mathrm T}.
	$$
	Таким образом, получилось проектирование на пространство синусов и косинусов, если разложить комплексную экспоненту. 
	Если брать всю матрицу $\mathrm X$, выйдет $K$ столбцов, спроектированных на данное пространство.
\end{proof}
\begin{comment}
	В \ref{subsec:cissa_fourier} рассмотрена связь между матрицей $\mathbf X_{B_k}$ и разложениями Фурье для векторов вложения.
\end{comment}

\noindent \textbf{\large{Нестационарный случай}} \newline \newline 
Для применения данного алгоритма на нестационарных временных рядах, нужно применить процедуру расширения ряда. Как утверждается авторами статьи \cite{bogalo2020}, после расширения, $\CISSA$ можно применить к нестационарному ряду.
Сама процедура расширения ряда $\TS$ производится с использованием авторегрессионной (AR) модели. Эта процедура позволяет предсказать значения временного ряда за его пределами (экстраполяция) как в правом, так и в левом направлениях на заданное число шагов $H$. Таким образом, трендовая (нелинейная) компонента ряда будет выделяться заметно лучше. В ходе работы алгоритм выполняет следующие шаги:
\begin{enumerate}
	\item \textbf{Определение порядка AR-модели}:  
	Метод определяет порядок $p$ AR-модели как целую часть от деления длины ряда $N$ на 3. Это значение порядка модели $p$ будет использовано для построения авторегрессионной модели на дифференцированном временном ряде;
	
	\item \textbf{Построение дифференцированного ряда}:  
	Временной ряд $\TS$ сначала преобразуется в дифференцированный ряд $d \TS$, чтобы удалить трендовые компоненты;
	
	\item \textbf{Построение AR-модели}:  
	После этого для дифференцированного ряда вычисляются коэффициенты авторегрессионной модели $A$ с использованием метода Юла-Уокера, основываясь на определенном ранее порядке $p$;
	
	\item \textbf{Правое расширение ряда}:  
	С помощью AR-модели ряд $d\TS$ прогнозируется на $H$ шагов вправо. Затем возвращается к своему изначальному состоянию путем интегрирования $d\TS$. Получается расширение исходного ряда $\TS$ на $H$ шагов вправо;
	
	\item \textbf{Левое расширение ряда}:  
	Аналогично предыдущему пункту, ряд прогнозируется на $H$ шагов влево;
	
	\item \textbf{Возвращение расширенного ряда}:  
	В конце метод возвращает расширенный временной ряд $\TS_{\mathrm{extended}}$, который содержит как левое, так и правое расширение на $H$ шагов от исходного ряда $\TS$.
\end{enumerate}


Таким образом, алгоритм расширения ряда позволяет выполнять предсказания временного ряда по обе стороны от его границ, основываясь на авторегрессионной модели, построенной на дифференцированном ряде, что полезно для выделения тренда. Однако поскольку мы рассматриваем расширенный ряд, то и периодические компоненты будут строиться по нему. Поэтому в угоду лучшего выделения трендовой составляющей, будет несколько жертвоваться точность разделения периодических компонентов. 




\subsection{Свойства}

%\subsubsection{Асимптотическая эквивалентность методов}
%В статье \cite{bogalo2020} говорится, что асимптотически методы $\SSA$ и $\CISSA$ эквивалентны в случае стационарного ряда и в доказательство приводится теорема \ref{th:equiv}.
%
%\begin{definition}
%	Будем говорить, что методы $M_1$ и $M_2$ асимптотически эквивалентны, если их матрицы вложения $S_1$, $S_2$ асимптотически эквиваленты в смысле $\operatorname*{lim}\limits_{L\rightarrow\infty \, N\rightarrow\infty}\frac{\|{S_1-S_2}\|_F}{\sqrt{L}}=0$, при некоторой последовательности $L = L(N) $, $N \rightarrow \infty$, где $\|{\cdot}\|_F$ --- норма Фробениуса. Тогда $M_1 \sim M_2$, $S_1 \sim S_2$.
%\end{definition}
%
%\begin{theorem}
%	\label{th:equiv}
%	Пусть $\TS$ --- стационарный временной ряд.
%	Дана $L \times K$ траекторная матрица $\mathbf{X}$, определенная в \eqref{eq:X}. Пусть $S_B = \mathbf{X} \mathbf{X}^T / K$, $S_T$ --- матрица, определенная в \eqref{eq:tepl_mat}, $S_C$ --- матрица, определенная в \eqref{eq:circ_mat}. Тогда $S_B \sim S_T \sim S_C$.
%\end{theorem}
%
%\begin{proof}
%	Доказательство в источнике \cite{bogalo2020}.
%\end{proof}
%
%
%Теорема \ref{th:equiv} дает понимание похожих практических результатов при применении разных методов.

\subsubsection{Связь CiSSA с разложением Фурье}
\label{subsec:cissa_fourier}
Для описания конечных, но достаточно длинных рядов можно использовать разложение Фурье. Пусть $\TS = (x_1, \dots, x_n, \dots)$ — временной ряд
\begin{definition}
	Разложение
	\begin{equation}
		\label{eq:fourier}
		x_n = c_0 + \sum\limits_{k = 1}^{\lfloor \frac{N+1}{2} \rfloor}\left(c_k \cos(2\pi n k / N) + s_k \sin(2\pi n k / N) \right),
	\end{equation}
	где $1 \leq n \leq N$ и $s_{N/2} = 0 $ для четного N, называется разложением Фурье ряда $\TS$. 
\end{definition}

Таким образом, можно выделить компоненту ряда, отвечающую за частоту $w_k = \frac{k-1}{L}$, $k = 1:\lfloor \frac{N+1}{2} \rfloor$;

Алгоритм $\CISSA$ тесно связан с разложением Фурье. По замечанию \ref{comm:proector} видно, что при вычислении $\mathbf X_{B_k} = \mathbf X_k + \mathbf X_{L+2-k} = U_k U_k^H \mathbf X + U_{L+2-k} U_{L+2-k}^H \mathbf X$, воспроизводится разложение Фурье для $K$ векторов матрицы $\mathrm X$. Затем вычисляется диагональное усреднение $\mathbf X_{B_k}$. А именно, $\CISSA$ можно представить так:
\begin{enumerate}
	\item Вычисляем разложение Фурье для каждого вектора вложения $L$-траекторной матрицы $\mathbf{X}$, состоящей из $K = N - L + 1$ векторов. Получается $K$ разложений Фурье по частотам $w_k = \frac{k-1}{L}$, $1:\lfloor \frac{L+1}{2} \rfloor$;
	\item По получившимся разложениям Фурье усредняем значения для соответствующих $x_i$ и частот $w_k$.
\end{enumerate}

\subsubsection{Точная разделимость}
Поскольку данный метод является аналогом разложения Фурье, то в смысле сильной разделимости можно точно разделить ряд, в котором одной из компонентов является $\cos(2\pi w + \varphi)$ с частотой $w$ такой, что $Lw = k \in \mathbb N$, или константа. Поэтому до применения алгоритма необходимо выделить интересующие частоты, то есть знать их заранее, и, исходя из них, выбирать значение $L$.

\subsubsection{Асимптотическая разделимость}
Асимптотическая разделимость в данном случае будет означать, что при увеличении $L$ разбиение сетки будет увеличиваться, а значит, и частоты в сетке начнут сближаться к истинным частотам периодических компонентов (либо становиться равными им), что будет снижать ошибку вычислений.

То есть, в случае непопадания периода определенной компоненты в разбиение частот алгоритма, будет выполняться $\CISSA$-асимптотическая $L(N)$-разделимость по определению \ref{def:asymp}. 



\newpage


\section{Метод Generalized singular spectrum analysis (GSSA)}
\label{sec:gssa}

В этом разделе описана модификация $\SSA$ на основе добавления весов к строкам $L$-траекторная матрица $\mathbf{X}$ \cite{gu2024generalized}. Авторы метода называют его обобщенным, поскольку базовый $\SSA$ является частным случаем $\GSSA$ с параметром $\alpha = 0$.

\subsection{Алгоритм метода GSSA}
Алгоритм $\GSSA$ сильно схож с базовым $\SSA$. Пусть $N > 2$, вещественнозначный временной ряд
$\TS = (x_1, \dots, x_{N})$ длины $N$. Фиксируется параметр $\alpha \geq 0$, отвечающий за веса:
\begin{equation*}
	{\boldsymbol{w}}^{(a)} = (w_{1}, w_{2}, \ldots, w_{L}) = \left( \left| \sin\left(\frac{\pi n}{L+1}\right) \right| \right)^\alpha, \quad \text{для } \quad n = 1, 2, \dots, L.
\end{equation*}

\subsubsection{Вложение}
$L$ --- некоторое целое число (длина окна), $1 < L < N$. Строится $L$-траекторная матрица $\mathbf{X}^{(\alpha)}$:
\begin{equation}
	\label{eq:X_alpha}
	\mathbf{X}^{(\alpha)} = 
	\begin{pmatrix}
		w_1 x_1 & w_1 x_2 & w_1 x_3 & \dots & w_1 x_{K} \\
		w_2 x_2 & w_2 x_3 & w_2 x_4 & \dots & w_2 x_{K+1} \\
		w_3 x_3 & w_3 x_4 & w_3 x_5 & \dots & w_3 x_{K+2} \\
		\vdots & \vdots & \vdots & \ddots & \vdots \\
		w_L x_{L} & w_L x_{L+1} & w_L x_{L+2} & \dots & w_L x_{N}
	\end{pmatrix}.
\end{equation}

\subsubsection{Сингулярное разложение (SVD)}
Этот шаг такой же, как и в $\SSA$, только матрица $\mathbf{X}$ заменяется на $\mathbf{X}^{(\alpha)}$. Будем обозначать собственные тройки в этом случае так: $(\sqrt{\lambda^{(\alpha)}},\,U^{(\alpha)},\,V^{(\alpha)})$.

\subsubsection{Группировка}
В точности как в $\SSA$. Тем самым, разложение может быть записано в сгруппированном виде:
\begin{equation*}
	\mathbf{X}^{(\alpha)} = \mathbf{X}^{(\alpha)}_{I_1} + \dots + \mathbf{X}^{(\alpha)}_{I_m}.
\end{equation*}

\subsubsection{Взвешенное диагональное усреднение}
Поскольку траекторная матрица была изменена весами, то диагональное усреднение тоже будет зависеть от весов.

Пусть $\mathbf{Y}$ --- матрица размерности $L \times K$. Взвешенное диагональное усреднение переводит матрицу $\mathbf{Y}$ в временной ряд $g_0, \dots, g_{N-1} $:

\begin{equation*}
	g_{k}=
	\begin{cases}
		\frac{1}{\sum_{n = 1}^k w_n} \sum\limits_{m=1}^{k+1} y_{m,k-m+2}^{*} &
		\text{для } 0 \leq k < L - 1, \\
		
		\frac{1}{\sum_{n = 1}^L w_n} \sum\limits_{m=1}^{L} y_{m,k-m+2}^{*} &
		\text{для } L-1 \leq k < K , \\
		
		\frac{1}{\sum_{n = k-K+1}^L w_n} \sum\limits_{m=k-K+2}^{N-K+1} y_{m,k-m+2}^{*} &
		\text{для } K \leq k < N .\\
	\end{cases}
\end{equation*}
Применяя данную операцию к матрицам $\mathbf{X^{(\alpha)}_{I_1}}, \dots, \mathbf{X^{(\alpha)}_{I_m}}$, получаются $m$ новых рядов: $\TS^{(\alpha)}_1, \dots, \TS^{(\alpha)}_m$. При этом, $\TS^{(\alpha)}_1 + \dots + \TS^{(\alpha)}_m = \TS^{(\alpha)}$.

\subsection{Свойства GSSA}
\subsubsection{Ранг ряда}
Зафиксируем ряд $\TS = (x_1, \dots, x_{N})$ длины $N > 3$ и длину окна $L$. 

Рассмотрим базовый $\SSA$. В процессе процедуры вложения получаем последовательность векторов вложения:
\begin{equation*}
	\mathrm{X}_i^{(L)} = \mathrm{X}_i = (x_{i-1}, \dots, x_{i+L-2}), \quad i = 1, \dots, K,
\end{equation*}
$\mathcal{L}^{(L)} = \mathcal{L}^{(L)}(\TS) \stackrel{{\rm def}}{{=}} \operatorname{span}(\mathrm X_{1}, \ldots, \mathrm X_{K})$ --- траекторное пространство ряда $\TS$. 
При этом, если $\dim \mathcal{L}^{(L)}= \operatorname{rank} \mathbf X = d$, то будем говорить, что ряд $\TS$ имеет $L$-ранг $d$ и записывать это как $\operatorname{rank}_L = d$.

Теперь рассмотрим $\GSSA$ и поймем, что для того же ряда $\operatorname{rank} \mathbf{X}^{(\alpha)} = \operatorname{rank} \mathbf{X}$, а значит, что для $\GSSA$ также применимы понятия $L$-ранга ряда. Из вида \eqref{eq:X_alpha} $\mathbf{X}^{(\alpha)}$ можно получить, что $\mathbf{X}^{(\alpha)} = \operatorname{diag}\left(w_1, w_2, \dots, w_L \right) \mathbf{X} = \operatorname{diag}\left({\boldsymbol{w}}^{(a)}\right) \mathbf{X}$. Поскольку матрица $\operatorname{diag}\left({\boldsymbol{w}}^{(a)}\right)$ имеет ранг равный $L$, она диагональна, то и $\operatorname{rank} \mathbf{X}^{(\alpha)} = \operatorname{rank} \operatorname{diag}\left({\boldsymbol{w}}^{(a)}\right)\mathbf{X} = \operatorname{rank} \mathbf{X}$.

\subsubsection{GSSA как линейный фильтр}
Аналогично $\SSA$, метод $\GSSA$ можно переписать с помощью линейных фильтров.
Пусть $\TS = (x_1, \dots, x_{N})$ --- временной ряд длины $N$, $K = N - L + 1, \quad L^{*} = \min(L, K)$. Пусть $L$ будет длиной окна, а $(\sqrt{\lambda^{(\alpha)}},\,U^{(\alpha)},\,V^{(\alpha)})$ — одной из собственных троек. Определим диагональную матрицу $N \times N$:
$$
\mathbf{D}^{(\alpha)} = \text{diag}(w_1, w_1 + w_2, \ldots,
 \sum \limits_{i = 1}^{L^*-1}w_i,
  \sum \limits_{i = 1}^{L^*}w_i, \sum \limits_{i = 1}^{L^*}w_i, \ldots, \sum \limits_{i = 1}^{L^*}w_i,
   \sum \limits_{i = 2}^{L^*}w_i, \ldots, w_{L^*-1}+ w_{L^*}, w_{L^*})
$$
и две матрицы  $K \times N$:
\[
\mathbf{W}^{(\alpha)} = \begin{pmatrix}
	u_{1}^{(\alpha)} & u_{2}^{(\alpha)} & u_{3}^{(\alpha)} & \cdots & u_{L}^{(\alpha)} & 0 & \cdots & 0 & 0 & 0 \\
	0 & u_{1}^{(\alpha)} & u_{2}^{(\alpha)} & u_{3}^{(\alpha)} & \cdots & u_{L}^{(\alpha)} & 0 & \cdots & 0 & 0 \\
	\vdots & 0 & \ddots & \ddots & \ddots & \cdots & \ddots & 0 & \cdots & 0 \\
	0 & \cdots & 0 & u_{1}^{(\alpha)} & u_{2}^{(\alpha)} & u_{3}^{(\alpha)} & \cdots & u_{L}^{(\alpha)} & 0 & \vdots \\
	0 & 0 & \cdots & 0 & u_{1}^{(\alpha)} & u_{2}^{(\alpha)} & u_{3}^{(\alpha)} & \cdots & u_{L}^{(\alpha)} & 0 \\
	0 & 0 & 0 & \cdots & 0 & u_{1}^{(\alpha)} & u_{2}^{(\alpha)} & u_{3}^{(\alpha)} & \cdots & u_{L}^{(\alpha)}
\end{pmatrix},
\]
\[
\mathbf{W}_{\boldsymbol{w}}^{(\alpha)} = \begin{pmatrix}
	w_1 u_{1}^{(\alpha)} & w_2 u_{2}^{(\alpha)} & w_3 u_{3}^{(\alpha)} & \cdots & w_L u_{L}^{(\alpha)} & 0 & \cdots & 0 & 0 & 0 \\
	0 & w_1 u_{1}^{(\alpha)} & w_2 u_{2}^{(\alpha)} & w_3 u_{3}^{(\alpha)} & \cdots & w_L u_{L}^{(\alpha)} & 0 & \cdots & 0 & 0 \\
	\vdots & 0 & \ddots & \ddots & \ddots & \cdots & \ddots & 0 & \cdots & 0 \\
	0 & \cdots & 0 & w_1 u_{1}^{(\alpha)} & w_2 u_{2}^{(\alpha)} & w_3 u_{3}^{(\alpha)} & \cdots & w_L u_{L}^{(\alpha)} & 0 & \vdots \\
	0 & 0 & \cdots & 0 & w_1 u_{1}^{(\alpha)} & w_2 u_{2}^{(\alpha)} & w_3 u_{3}^{(\alpha)} & \cdots & w_L u_{L}^{(\alpha)} & 0 \\
	0 & 0 & 0 & \cdots & 0 & w_1 u_{1}^{(\alpha)} & w_2 u_{2}^{(\alpha)} & w_3 u_{3}^{(\alpha)} & \cdots & w_L u_{L}^{(\alpha)}
\end{pmatrix}.
\]
Здесь $U = (u_1, \dots, u_L)$ --- собственный вектор матрицы $\mathbf{S}$.
\begin{theorem}
	\label{th:filter_GSSA}
	Компонента временного ряда $\widetilde \TS$, восстановленная с использованием собственной тройки $(\sqrt{\lambda^{(\alpha)}},\,U^{(\alpha)},\,V^{(\alpha)})$, имеет вид:
	\[
	\widetilde{\TS}^{{\rm T}} = {\mathbf{D}^{(\alpha)}}^{-1}
	{\mathbf{W}^{(\alpha)}}^{{\rm T}}
	\mathbf{W}_{\boldsymbol{w}}^{(\alpha)}
	\TS^{\rm T}.
	\]
\end{theorem}
\begin{proof}
	Доказательство аналогично $\ref{th:filter_SSA}$.
\end{proof}

Таким образом, для восстановления методом $\GSSA$ средних точек (индексы от $L$ до $K$) имеем следующий фильтр:
\begin{equation}
	{\widetilde{x}}_{s} = \sum_{j=-(L-1)}^{L-1} \left( \sum_{k=1}^{L-|j|} u_{k}^{(\alpha)} u_{k+|j|}^{(\alpha)} w_k / \sum\limits_{i = 1}^{L}w_i \right) x_{s-j}, \quad L \leq s \leq K.
\end{equation}
Похожим образом можно переписать $\GSSA$ через линейные фильтры для точек в начале и конце.


\newpage

\section{Сравнение алгоритмов разложения Фурье, SSA и CiSSA}
\label{sec:comparison}
%План:
%\begin{enumerate}
%	\item Про базис: SSA --- адаптивный базис (?), CiSSA --- неадаптивный(Фурье разложение).
%	\item Классы точной разделимости: (? их не так много в обоих случаях)
%	\item Классы асимптотической разделимости: SSA --- $x^n$, cos, exp, cos exp; CiSSA разделит cos, остальное смешает между трендом, шумом и теми же косинусами. Подтвердить вычислениями.
%	\item Указать на автоматическое выделение частот в CiSSA, как достоинство, тут же посмотреть на автогруппировку в SSA. Рассмотреть модельные примеры и данные IP.
%	\item Посмотреть, что будет, если частота cos не попадёт в диапазон частот при разложении CiSSA. 
%\end{enumerate}	
Все вычисления, а также код $\CISSA$ можно найти в github репозитории \cite{spbu_cissa_coursework_github}.

\subsection{Преимущества и недостатки методов}
В данной секции проводится сравнение различных методов: базовый $\SSA$,  $\SSA$ с использованием EOSSA для улучшения разделимости, разложения Фурье и Фурье с расширением ряда, базового $\CISSA$ и $\CISSA$ с расширением ряда. Для наглядного отображения преимуществ каждого из этих методов составлена таблица \ref{tab:advantages}, где строки соответствуют методам, а столбцы --- условиям (особым видам компонент ряда). На пересечении строк и столбцов указан знак, показывающий, достигается ли разделение компоненты: плюс (+) обозначает точное выполнение, знак стремления указывает на асимптотическое выполнение, а минус (–) --- на отсутствие разделимости. Для разложения Фурье подразумевается, что $L = N$. 

Обозначения: 
\begin{itemize}
	\item $\cos$ --- в ряде присутствуют только периодические компоненты вида $\cos(2\pi\omega x + \varphi)$;
	\item $\TS_{\mathrm{np}1}$ --- одна непериодическая компонента в ряде, остальные имеют период;
	\item $\TS_{\mathrm{np}}$ --- несколько непериодических компонент в ряде, остальные имеют период, интересует разделение между
	непериодическими компонентами;
	\item group --- автоматическая группировка по заданным частотам.
\end{itemize}
 
 
\begin{table}[H]
	\centering
	\begin{center}
	\begin{tabular}{ccccccccc}
		\hline
		Метод/Условие  &|& $\cos$,                 & $\cos$,                    & $\cos$,                     & $\TS_{\mathrm{np1}}$   & $\TS_{\mathrm{np}}$ & group\\ 
		               &|& $Lw = k \in \mathbb N$, & $Lw = k \in \mathbb N$,    & $Lw = k \not\in \mathbb N$, &             \\
		               &|& $Kw = k \in \mathbb N$  & $Kw = k \not\in \mathbb N$ & $Kw = k \not\in \mathbb N$  &             \\ 
		\hline
		SSA            &|& $+$                     & $\to$                      & $\to$                       & $\to$ & $\to$ & $-$ \\
		SSA EOSSA      &|& $+$                     & $\to$                      & $\to$                       & $\to$ & $\to$ & $+$ \\
		Fourier        &|& $+$                     & $+$                        & $\to$                       & $-$   & $-$ & $+$ \\
		Fourier extended        &|& $+$                     & $+$                        & $\to$                       & $-$   & $-$ & $+$ \\
		CiSSA          &|& $+$                     & $+$                        & $\to$                       & $-$   & $-$ & $+$ \\
		CiSSA extended &|& $+$                     & $+$                        & $\to$                       & $\to$ & $-$ & $+$ \\
		\hline
	\end{tabular}
	\end{center}
	\caption{Преимущества и недостатки методов} 
	\label{tab:advantages}
\end{table}


На основе таблицы \ref{tab:advantages} были выбраны примеры, следующие ниже.

Данные методы разложения временного ряда должны совпадать, если ряд состоит только из периодических компонент. Например, пусть $\TS = \TS_{\sin} + \TS_{\cos} = \sin{\frac{2\pi}{12}x} + \frac{1}{2}\cos{\frac{2\pi}{3}x}$, $L = 96$, $N = 96 \cdot 2$ для разложения Фурье и $N = 96 \cdot 2 - 1$ для остальных, чтобы выполнялись условия выполнения разделимости частот. Сравним результаты по среднеквадратичной ошибке:

\begin{table}[H]
	\centering
	\begin{tabular}{llllllll}
		\hline
		Метод/Компонента & $\TS_{\sin}$ & $\TS_{\cos}$ \\ 
		\hline
		SSA & 6.8e-30 & 1.5e-29 \\ 
		SSA EOSSA & 1.5e-29 & 7.5e-30 \\ 
		Fourier & 1.7e-28 & 3.5e-28 \\ 
		Fourier extended & 6.2e-04 & 2.6e-03 \\ 
		CiSSA & 1.9e-29 & 5.3e-30 \\ 
		CiSSA extended & 2.0e-04 & 8.6e-04 \\ 
		\hline
	\end{tabular}
	\caption{MSE разложений ряда $\TS = \TS_{\sin} + \TS_{\cos}$ методов} 
	\label{tab:errs_fourier_cissa_sin_cos}
\end{table}

Таблица \ref{tab:errs_fourier_cissa_sin_cos} показывает, что разложения без расширений ряда сделали правильное (с точностью до вычислений с помощью компьютера) разделение компонент ряда. Однако расширение в методах $\CISSA$ и Фурье ухудшило разделимость периодических частей.

Теперь добавим к этому ряду шум: $\TS = \TS_{\sin} + \TS_{\cos} + \TS_{\mathrm{noise}} = \sin{\frac{2\pi}{12}x} + \frac{1}{2}\cos{\frac{2\pi}{3}x} + \varepsilon_n$, где $\varepsilon_n \sim \mathrm N(0, 0.1)$, $L = 96$, $N = 96 \cdot 2$ для разложения Фурье и $N = 96 \cdot 2 - 1$ для остальных. Результаты должны ухудшиться. Проводилось $100$ тестов, в таблице \ref{tab:errs_fourier_cissa_sin_cos_noised} указаны средние значения ошибки для одних и тех же реализаций шума.
\begin{table}[H]
	\centering
	\begin{tabular}{llllllll}
		\hline
		Метод/Компонента & $\TS_{\sin}$ & $\TS_{\cos}$ \\ 
		\hline
		SSA & 2.9e-04 & 3.1e-04 \\ 
		SSA EOSSA & 2.9e-04 & 3.1e-04 \\ 
		Fourier & 1.0e-04 & 1.1e-04 \\ 
		Fourier extended & 1.3e-03 & 3.9e-03 \\ 
		CiSSA & 1.6e-04 & 1.8e-04 \\ 
		CiSSA extended & 6.6e-04 & 1.9e-03 \\ 
		\hline
	\end{tabular}
	\caption{MSE разложений ряда $\TS = \TS_{\sin} + \TS_{\cos} +\TS_{\mathrm{noise}}$ методов} 
	\label{tab:errs_fourier_cissa_sin_cos_noised}
\end{table}

По таблице \ref{tab:errs_fourier_cissa_sin_cos_noised} видно, что зашумление ряда дало негативный эффект на ошибку. Также был проведен парный t-критерий для зависимых выборок с целью проверки гипотезы о равенстве средних значений ошибки для каждой компоненты, попарно для всех методов. В качестве нулевой гипотезы ($H_0$) предполагалось, что средние значения двух сравниваемых выборок равны. Критический уровень значимости был установлен на уровне $\alpha = 0.05$.
Результаты анализа показали, что во всех случаях $p$-значение оказалось меньше 0.05, что позволяет отвергнуть нулевую гипотезу.


Попробуем добавить к ряду непериодическую компоненту. $\TS = \TS_{\sin} + \TS_{\cos} + \TS_{c} + \TS_e = \sin{\frac{2\pi}{12}x} + \frac{1}{2}\cos{\frac{2\pi}{3}x} + 1 + e^{\frac{x}{100}}$, $L = 96$, $N = 96 \cdot 2$ для разложения Фурье и $N = 96 \cdot 2 - 1$. Непериодические компоненты будут отвечать низким частотам. Проблема лишь в том, что с помощью методов разложения Фурье $\CISSA$ невозможно различить между собой две непериодические компоненты, поскольку группировка работает по частотам, элементы разложения неизбежно смешаются между собой. Будем искать экспоненту и константу по низким частотам, назовем это трендовой составляющей ряда. По таблице \ref{tab:advantages} лучше всех должен справиться $\SSA$ с улучшением разделимости EOSSA. Хуже всех --- разложение Фурье, поскольку он никаким образом не сможет вычленить из ряда экспоненту.

\begin{table}[H]
	\centering
	\begin{tabular}{llllllll}
		\hline
		Метод/Компонента & $\TS_{c} + \TS_e$ & $\TS_{\sin}$ & $\TS_{\cos}$ \\ 
		\hline
		SSA & 5.0e-03 & 8.9e-07 & 5.2e-05 \\ 
		SSA EOSSA & 1.7e-28 & 1.6e-29 & 8.7e-30 \\ 
		Fourier & 1.1e-01 & 6.1e-04 & 6.8e-03 \\ 
		Fourier extended & 1.4e-03 & 1.3e-03 & 8.4e-03 \\ 
		CiSSA & 5.3e-02 & 1.6e-05 & 4.9e-04 \\ 
		CiSSA extended & 5.0e-04 & 2.1e-04 & 1.1e-03 \\ 
		\hline
	\end{tabular}
	\caption{MSE разложений ряда $\TS = \TS_{\sin} + \TS_{\cos} + \TS_{c} + \TS_e$ методов} 
	\label{tab:errs_fourier_cissa_trend}
\end{table}

Результаты таблицы \ref{tab:errs_fourier_cissa_trend} повторяют вышеизложенные рассуждения. Также заметно, что периодические компоненты лучше выделились с помощью $\CISSA$ без процедуры расширения ряда в сравнении с $\CISSA$ с расширением.

Теперь добавим шум в предыдущий пример. Результаты всех разложений должны ухудшиться. $\TS = \TS_{\sin} + \TS_{\cos} + \TS_{c} + \TS_e + \TS_{\mathrm{noise}} = \sin{\frac{2\pi}{12}x} + \frac{1}{2}\cos{\frac{2\pi}{3}x} + 1 + e^{\frac{x}{100}} + \mathrm N(0, 0.1)$, $L = 96$, $N = 96 \cdot 2$ для разложения Фурье и $N = 96 \cdot 2 - 1$. Было проведено 100 тестов, в таблице \ref{tab:errs_fourier_cissa_trend_noised} указаны средние значения ошибки.

\begin{table}[H]
	\centering
	\begin{tabular}{llllllll}
		\hline
		Метод/Компонента & $\TS_{\sin}$ & $\TS_{\cos}$ & $\TS_{c} + \TS_e$\\ 
		\hline
		SSA & 2.9e-04 & 3.6e-04 & 5.2e-03 \\ 
		SSA EOSSA & 2.9e-04 & 3.1e-04 & 9.4e-04 \\ 
		Fourier & 6.9e-04 & 7.2e-03 & 1.2e-01 \\ 
		Fourier extended & 1.9e-03 & 9.6e-03 & 3.0e-03 \\ 
		CiSSA & 1.7e-04 & 7.0e-04 & 5.5e-02 \\ 
		CiSSA extended & 6.8e-04 & 2.1e-03 & 2.7e-03 \\ 
		\hline
	\end{tabular}
	\caption{MSE разложений ряда $\TS = \TS_{\sin} + \TS_{\cos} + \TS_{c} + \TS_e+\TS_{\mathrm{noise}}$ методов} 
	\label{tab:errs_fourier_cissa_trend_noised}
\end{table}

Как видно из таблицы \ref{tab:errs_fourier_cissa_trend_noised}, разделения ухудшились, однако $\SSA$ с улучшением разделимости EOSSA отработал лучше всех. Также был проведен был проведён двухвыборочный t-критерий для зависимых выборок с целью проверки гипотезы о равенстве средних значений ошибки для каждой компоненты, попарно для всех методов. В качестве нулевой гипотезы ($H_0$) предполагалось, что средние значения двух сравниваемых выборок равны. Критический уровень значимости был установлен на уровне $\alpha = 0.05$.
Результаты анализа показали, что во всех случаях $p$-значение оказалось меньше 0.05, что позволяет отвергнуть нулевую гипотезу.

По результатам данных примеров и таблицы \ref{tab:advantages}, можно понять, что $\CISSA$ работает лучше, чем разложение Фурье как при расширении ряда, так и без него. Однако это не удивительно, ведь разложение Фурье это частный случай $\CISSA$ при $L = N$. А $\SSA$ с улучшением разделимости EOSSA показал себя лучше базового $\SSA$. Таким образом, далее не будем рассматривать разложение Фурье и базовый $\SSA$, остановимся на $\SSA$ с EOSSA, $\CISSA$ с расширением и без него. Кроме того, по умолчанию будет использоваться $\CISSA$ с расширением, если есть непериодичность, и обычный $\CISSA$, если все компоненты периодичны. Также при написании $\SSA$ будет подразумеваться использование $\SSA$ с EOSSA, если нет конкретных указаний.




\subsection{Собственные пространства}
Каждый алгоритм после группировки порождает построенными матрицами собственные подпространства. В случае базового $\SSA$ алгоритма базис подпространств является адаптивным, то есть зависящим от $\TS, L, N$. Таким образом, $\SSA$ может отличить, например, произведение полиномов, экспонент и косинусов друг от друга.

В случае $\CISSA$ базис зависит только от $L, N$. Если зафиксировать данные параметры, и менять $\TS$, базис никак не поменяется.
	

\subsection{Точная разделимость}
Как удалось выяснить, классов точной разделимости больше в базовом алгоритме $\SSA$, однако в случае разделения $\cos$, условия менее жесткие при использовании $\CISSA$.

Проверим на примерах. 
Возьмем временной ряд, с разложением которого оба алгоритма должны справиться: $\TS = \TS_{C} + \TS_{cos} = 1 + \cos(\frac{2\pi}{12}x)$, $L = 96 \mid 12$, $N = 96 \cdot 2-1$, $K = 96 \mid 12$. Будем считать MSE между настоящими компонентами ряда и вычисленными.
\begin{table}[H]
	\centering
	\begin{tabular}{llllllll}
		\hline
		Метод/Компонента & $\TS_{C}$ & $\TS_{\cos} $\\ 
		\hline
		SSA & 2.1e-30 & 4.9e-30 \\ 
		CiSSA & 3.6e-31 & 5.2e-30 \\ 
		
		\hline
	\end{tabular}
	\caption{MSE разложений ряда $\TS = \TS_{C} + \TS_{\cos}$, $\omega K \in \mathbb N$.} 
	\label{tab:4_2_1}
\end{table}
Ошибки таблицы \ref{tab:4_2_1} можно посчитать за погрешность вычислений на компьютере.

Теперь возьмем временной ряд, при котором $\SSA$ должен отработать хуже $\CISSA$: $\TS = \TS_{C} + \TS_{\cos} = 1 + cos(\frac{2\pi}{12}x)$, $L = 96 \mid 12$, $N = 96 \cdot 2+5$, $K = 102 \nmid 12$. Поскольку $K$ не делится на частоту косинуса, условия точной разделимости в $\SSA$ не выполняются. Будем считать MSE между настоящими компонентами ряда и вычисленными.
\begin{table}[H]
	\centering
	\begin{tabular}{llllllll}
		\hline
		Метод/Компонента & $\TS_{C}$ & $\TS_{\cos} $\\ 
		\hline
		SSA & 9.5e-5 & 9.6e-5 \\ 
		CiSSA & 3.2e-31 & 5.1e-30 \\ 
		
		\hline
	\end{tabular}
	\caption{MSE разложений ряда $\TS = \TS_{C} + \TS_{\cos}$, $\omega K \not \in \mathbb N$.} 
	\label{tab:4_2_2}
\end{table}
Таким образом, с разделением косинуса от константы лучше справился алгоритм $\CISSA$, поскольку в нем требуется меньше условий на параметры алгоритма.

\subsection{Асимптотическая разделимость}
Как было сказано, асимптотически разделимы в методе $\SSA$ полиномы, гармонические функции (косинус, косинус помноженный на экспоненту, экспонента) \cite{golyandina2001analysis}. 
В алгоритме $\CISSA$ при увеличении длины окна $L$ меняется сетка разбиения частот. Из-за этого, даже если не удастся выбрать подходящее $L$, при котором будет точно отделим косинус, но постоянно его увеличивать, в конечном счете получится снизить ошибку выделения нужной компоненты косинуса, если брать соседние частоты с частотой компоненты. Однако в этом случае нужно выбирать диапазон частот, которые стоит объединить.

Непериодические компоненты повлияют на ошибку разложений всего временного ряда, они смешаются и их уже никак не получится отделить методом $\CISSA$. Рассмотрим более детально пример с экспонентой: $\TS = \TS_c + \TS_e + \TS_{\cos} + \TS_{\sin} = 1 + e^{\frac{x}{100}} + \cos(\frac{2\pi}{12} x) + \sin(\frac{2\pi}{24} x)$, $N = 96 \cdot 2 - 1, \, L = 96$, можно получить следующие результаты:

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{img/trend inseparability example/all.png}
	\caption{Правильное разложение ряда $\TS = \TS_c + \TS_e + \TS_{\cos} + \TS_{\sin}$}
	\label{fig:c_e_cos}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{img/trend inseparability example/ssa.png}
	\caption{Разложение ряда $\TS = \TS_c + \TS_e + \TS_{\cos} + \TS_{\sin}$ методом $\SSA$}
	\label{fig:c_e_cos_ssa}
\end{figure}

Метод $\SSA$ разделил правильно все компоненты друг от друга.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{img/trend inseparability example/cissa.png}
	\caption{Разложение ряда $\TS = \TS_c + \TS_e + \TS_{\cos} + \TS_{\sin}$ методом $\CISSA$}
	\label{fig:c_e_cos_cissa}
\end{figure}

В случае $\CISSA$ получилось так, что экспонента и константа смешались в одну компоненту. Как и в примере сравнении разделения ряда Фурье и $\CISSA$, одни и те же частоты отвечают одновременно и за константу, и за экспоненту.

% latex table generated in R 4.2.2 by xtable 1.8-4 package
% Wed Jun 12 06:57:37 2024
\begin{table}[H]
	\centering
	\begin{tabular}{llllllll}
		\hline
		Метод/Компонента &|& $\TS_e$ & $\TS_c$ & $\TS_c + \TS_e$ &|& $\TS_{\sin}$ & $\TS_{\cos}$ \\ 
		\hline
		SSA & |&2.2e-25 & 2.2e-25 & 4.2e-28 &|&3.8e-29 & 1.6e-29 \\ 
		CiSSA &| &none & none & 3.5e-02 & |&1.4e-04 & 1.9e-03 \\ 
		\hline
	\end{tabular}
	\caption{MSE разложений ряда $\TS = \TS_c + \TS_e + \TS_{\cos} + \TS_{\sin}$ методов $\SSA$ и \CISSA} 
	\label{tab:errs}
\end{table}

Таблица \ref{tab:errs} и рисунки \ref{fig:c_e_cos_ssa}, \ref{fig:c_e_cos_cissa} показывают, что метод $\SSA$ справился лучше в сравнении с $\CISSA$, причем как по разделимости, так и по ошибке. В алгоритме $\CISSA$ трендовая составляющая также смешалась с сезонной, поэтому увеличилась ошибка при косинусе. Стоит отметить, что в данном примере использовался алгоритм улучшения разделимости EOSSA \cite{golyandina2023intelligent} для метода $\SSA$. Без него не получились бы такие результаты.

Или же, если заменить $\TS_{e}$ на $\TS_{e \cdot \cos}$, то есть теперь ряд $\TS = \TS_c + \TS_{e \cdot \cos} + \TS_{\cos} + \TS_{\sin} = 1 + e^{\frac{x}{100}} \cos(\frac{2\pi}{48} x) + \cos(\frac{2\pi}{12} x) + \sin(\frac{2\pi}{24} x)$, то получится следующая таблица ошибок:
\begin{table}[H]
	\centering
	\begin{tabular}{llllllll}
		\hline
		Метод/Компонента &|& $\TS_{e \cdot \cos}$ & $\TS_{\sin}$ & $\TS_{\cos}$ \\ 
		\hline
		SSA & |&4.7e-29 & 1.1e-29 & 8.4e-30 \\ 
		CiSSA &| &3.2e-02 & 2.6e-04 & 5.8e-03 \\ 
		\hline
	\end{tabular}
	\caption{MSE разложений ряда $\TS = \TS_c + \TS_{e \cdot \cos} + \TS_{\cos} + \TS_{\sin}$ методов $\SSA$ и \CISSA} 
	\label{tab:errs_exp_cos}
\end{table}

Таким образом, таблица \ref{tab:errs_exp_cos} показывает тот же недостаток у метода $\CISSA$, что и таблица \ref{tab:errs}.

\subsection{Отделение сигнала от шума}
%TODO: везде указывать диапазоны частот
Рассматривая ряд из предыдущего пункта, добавим к нему гауссовский  шум с стандартным отклонением $0.1$: $\TS = \TS_c + \TS_e + \TS_{\cos} + \TS_{\sin} +\TS_{\mathrm{noise}} = 1 + e^{\frac{x}{100}} + \cos(\frac{2\pi}{12} x) + \sin(\frac{2\pi}{24} x) + \mathrm N(0, 0.1)$, $N = 96 \cdot 2 - 1, \, L = 96$. Сделав такой тест $10000$ раз, получим следующий результат по ошибке $\mathrm{MSE}$ между настоящим сигналом и его оценкой:
\begin{table}[H]
	\centering
	\begin{tabular}{llllllll}
		\hline
		Метод/Статистики &|& min & median & mean & max & sd\\ 
		\hline
		
		SSA &|& 5.8e-04 & 2.0e-03 & 2.1e-03 & 4.9e-03 & 6.2e-04\\ 
		CiSSA &|& 2.5e-02 & 3.4e-02 & 3.4e-02 & 4.9e-02 & 3.7e-03\\ 
		\hline
	\end{tabular}
	\caption{Данные по распределению ошибки восстановления сигнала разложений методов $\SSA$ и \CISSA} 
	\label{tab:errs_of_errs}
\end{table}


По таблице \ref{tab:errs_of_errs} можно увидеть что метод $\SSA$ отработал лучше $\CISSA$.



\subsection{Автоматическая группировка и проверка на реальных данных}
Авторы статьи \cite{bogalo2020} выделяют главным преимуществом то, что $\CISSA$ автоматически разделяет компоненты ряда по частотам. Однако есть метод, позволяющий сделать автоматическое объединение частот по периодограмме в методе $\SSA$ \cite{golyandina2023intelligent}. При этом, прежде чем применять его, стоит выполнить процедуру улучшения разделимости. В данной работе будут использоваться методы EOSSA и FOSSA \cite{golyandina2023intelligent}.

Сравним работы этих алгоритмов сначала на модельных примерах, затем на реальных данных. 

Используем те же данные, что и в прошлом примере: $\TS = \TS_c + \TS_e + \TS_{cos} = 1 + e^{\frac{x}{100}} + cos(\frac{2\pi}{12})$, $N = 96 \cdot 2 - 1, \, L = 96$. Применяем алгоритм EOSSA \cite{golyandina2023intelligent} для лучшей разделимости и выбираем в качестве интересующих частот диапазоны $\left(\frac{1}{24}-\varepsilon, \frac{1}{24}+\varepsilon \right), \left(\frac{1}{12}-\varepsilon, \frac{1}{12}+\varepsilon\right), \varepsilon = \frac{1}{97}$. Результаты остаются теми же, как и в таблице \ref{tab:errs} и рисунках \ref{fig:c_e_cos_ssa}, \ref{fig:c_e_cos_cissa}, однако теперь группировка ряда произошла по интересующим частотам.

Теперь рассмотрим реальные данные --- месячные ряды промышленного производства (Industrial Production, IP), index $2010 = 100$, в США. Данные промышленного производства полезны, поскольку оно указывается в определении рецессии Национальным бюро экономических исследований (NBER), как один из четырех ежемесячных рядов индикаторов, которые необходимо проверять при анализе делового цикла. Выборка охватывает период с января 1970 года по сентябрь 2014 года, поэтому размер выборки составляет $N = 537$. Источником данных является база данных IMF. Эти показатели демонстрируют различные тенденции, сезонность и цикличность (периодические компоненты, которые соответствуют циклам бизнеса). Данные IP также рассматривались в статье \cite{bogalo2020}. Применим как $\CISSA$, так и $\SSA$ с автоматическим определением частот и улучшением разделимости по следующим группам:
\begin{enumerate}
	\item Трендовой составляющей должны отвечать низкие частоты, поэтому диапазон: $\left[0, \frac{1}{192}\right]$;
	\item Циклы бизнеса по диапазонам: $\left[\frac{2}{192}, \frac{10}{192}\right]$;
	\item Сезонность по частотам $\omega_k = 1/12, 1/6, 1/4, 1/3, 5/12, 1/2$;
\end{enumerate}
На основе предыдущих требований взято $L = 192$.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{img/trend inseparability example/IP_trend.png}
	\caption{Трендовая составляющая данных IP USA}
	\label{fig:IP_trend}
\end{figure}

При применении FOSSA улучшения разделимости алгоритм $\SSA$ выделяет тренд довольно похоже с $\CISSA$. Весь график $\SSA$ тренд EOSSA выглядит более изогнутым при визуальном сравнении с остальными.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{img/trend inseparability example/IP_cycle.png}
	\caption{Циклическая составляющая данных IP USA}
	\label{fig:IP_cycle}
\end{figure}

Аналогичная тренду ситуация происходит с цикличностью. В случае EOSSA правый хвост (значения ряда после 2010-ого года) смешался между цикличностью и трендом.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{img/trend inseparability example/IP_sesonal.jpg}
	\caption{Сезонная составляющая данных IP USA}
	\label{fig:IP_sesonal}
\end{figure}

Поскольку в базовом $\SSA$ адаптивный базис, сезонность является менее систематичной, разброс значений выше по сравнению с $\CISSA$.

%\begin{figure}[H]
%	\centering
%	\includegraphics[width=0.8\textwidth]{img/trend inseparability example/IP_residuals.png}
%	\caption{Шум данных IP USA}
%	\label{fig:IP_residuals}
%\end{figure}
Шум же является нормальным во всех случаях.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{img/trend inseparability example/W-corr.jpg}
	\caption{Матрицы корреляций IP USA}
	\label{fig:W-corr}
\end{figure}
%TODO: по pdf
По матрицам корреляции заметно, что при использовании $\SSA$ с улучшением разделимости EOSSA, сильно смешиваются первые по значимости компоненты ряда (они и являются трендовыми и циклическими). 

%\textbf{ОТСЕБЯТИНА}
%Тренд наиболее гибко и лучше отделяется при применении SSA, цикличность отделилась одинаково в обоих случаях, сезонность выглядит куда приличнее при применении CiSSA. Плюс, приходится подбирать параметры разложения в SSA. В CiSSA вообще ничего не надо делать, просто вкинул, отработало замечательно. 


Таким образом, получились довольно похожие результаты в выделении тренда и цикличности при использовании $\SSA$ с FOSSA и $\CISSA$. Несколько иные результаты при $\SSA$ с EOSSA. Сезонная составляющая в силу неадаптивного базиса более строго выглядит для метода $\CISSA$.
 
 
\subsection{Выводы}
По полученным результатам, можно следующие выводы: 
\begin{enumerate}
	\item Алгоритм $\CISSA$ работает лучше разложения Фурье;
	\item Если понятно, что ряд состоит только из периодических компонент, стоит использовать $\CISSA$ без процедуры расширения, поскольку она делает ошибки разделений периодики больше. И напротив, если есть непериодичность, лучше расширять ряд;
	\item Если данные зашумлены или имеется непериодичность, алгоритм $\SSA$ с улучшением разделимости справляется в среднеквадратичном лучше $\CISSA$ с расширением ряда или без.
\end{enumerate}


%TODO: сделать расширение с рядом фурье 

\newpage

\section{Заключение}
\label{sec:concl}

В данной работе исследован алгоритм $\CISSA$, сравнены методы $\CISSA$ и $\SSA$, и полученные
знания были проверены на реальных и смоделированных примерах с помощью языка R. Оба алгоритма справляются с поставленными задачами, существенным различием является то, что алгоритм $\SSA$ является более гибким: в нем адаптивный базис, есть дополнительные алгоритмы, которые довольно похоже приближают этот алгоритм к $\CISSA$, а также методы для автоматического выбора компонентов по частотам. Метод $\CISSA$ является простым в использовании.


Дальнейшими действиями является рассмотрение других модификаций метода $\SSA$.


\newpage

\bibliographystyle{plain}
\bibliography{ref}


\end{document}

